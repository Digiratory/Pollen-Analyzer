{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2007520-3e57-4eec-8607-071051c4f634",
   "metadata": {},
   "source": [
    "It is the complete pipeline that is used in the paper \"Automatic online monitoring of allergic pollen\". \n",
    "\n",
    "This one consists of Instance segmentation and Feature-extracting models.\n",
    "\n",
    "For running the pipeline use weights from **url** and dataset from **url**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68711c1-99ce-447c-840b-156db4742948",
   "metadata": {},
   "source": [
    "## Instance segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903f275f-a450-4d07-91d9-7082d5c5a848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "fe46b2ac-d358-43cc-8d44-2ce8ca87f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Tuple\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "from torchvision import tv_tensors\n",
    "from torchvision import datasets\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.transforms.v2 import functional as TVF\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "22599ac9-427d-406c-825a-fe5b48b4683d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentImageDataset(datasets.DatasetFolder):\n",
    "    def __init__(self, object_dir: str, back_dir: str, package=100, back_usages=2, objects_per_image=(1, 2),\n",
    "                 transform=None, transform_object=None):\n",
    "        self.img_dir = object_dir\n",
    "        self.back_dir = back_dir\n",
    "        self.objects_per_image = objects_per_image\n",
    "        self.package = package\n",
    "        self.back_usages = back_usages\n",
    "        self.transform = transform\n",
    "        self.transform_object = transform_object\n",
    "        self.objects = []\n",
    "        self.backgrounds = []\n",
    "\n",
    "        for folder_name in os.listdir(object_dir):\n",
    "            folder_path = os.path.join(object_dir, folder_name)\n",
    "            for image_name in os.listdir(folder_path):\n",
    "                image = Image.open(os.path.join(folder_path, image_name)).convert('RGB')\n",
    "                self.objects.append([image, folder_name])\n",
    "\n",
    "        for image_name in os.listdir(back_dir):\n",
    "            image = TVF.to_pil_image(read_image(os.path.join(back_dir, image_name)))\n",
    "            self.backgrounds.append(image)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.backgrounds) * self.package * self.back_usages\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx //= self.package * self.back_usages\n",
    "        close_param = 0.4\n",
    "        back = deepcopy(self.backgrounds)[idx]\n",
    "        objects = deepcopy(self.objects)\n",
    "        objects_per_image = randint(*self.objects_per_image)\n",
    "\n",
    "        busy_places = []\n",
    "        target = {}\n",
    "        masks = []\n",
    "        itr = 0\n",
    "        while len(busy_places) < objects_per_image and itr < objects_per_image * 3:\n",
    "            itr += 1\n",
    "            object = objects[randrange(len(self.objects))]\n",
    "            object_image = TVF.to_pil_image(clean_background(np.array(object[0])))\n",
    "\n",
    "            if self.transform_object is not None:\n",
    "                object_image = self.transform_object(object_image)\n",
    "\n",
    "            mask_background = TVF.to_pil_image(np.zeros((back.size[1], back.size[0], 1)))\n",
    "            pos_x = randrange(0, back.size[0] - object_image.size[0])\n",
    "            pos_y = randrange(0, back.size[1] - object_image.size[1])\n",
    "            center_dot = ((pos_x + object_image.size[0]) // 2, (pos_y + object_image.size[1]) // 2)\n",
    "            radius = max(object_image.size) // 2\n",
    "\n",
    "            for bp in busy_places:\n",
    "                if ((bp[0][0] - center_dot[0]) ** 2 + (bp[0][1] - center_dot[1]) ** 2) ** 0.5 < (\n",
    "                        radius + bp[1]) * close_param:\n",
    "                    break\n",
    "            else:\n",
    "                mask_background.paste(object_image, (pos_x, pos_y), object_image)\n",
    "                mask = np.array(mask_background)\n",
    "                mask[mask != 0] = 1\n",
    "                masks += [mask] \n",
    "\n",
    "                busy_places.append((center_dot, radius))\n",
    "                back.paste(object_image, (pos_x, pos_y), object_image)\n",
    "\n",
    "        img = tv_tensors.Image(back)\n",
    "        masks = torch.from_numpy(np.array(masks)).to(dtype=torch.uint8)\n",
    "        labels = torch.ones((len(busy_places),), dtype=torch.int64)\n",
    "        ispollen = torch.zeros((len(busy_places),), dtype=torch.int64)\n",
    "        boxes = masks_to_boxes(masks)\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        \n",
    "        target['masks'] = tv_tensors.Mask(masks)\n",
    "        target['boxes'] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=TVF.get_size(back))\n",
    "        target['labels'] = labels\n",
    "        target['iscrowd'] = ispollen # easier to use iscrowd\n",
    "        target['area'] = area\n",
    "        target['image_id'] = idx\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img, target = self.transform(img, target)\n",
    "        \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "886418e6-ad82-408a-9020-d5cbc2a54399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_background(src):\n",
    "    tmp = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)\n",
    "    _,alpha = cv2.threshold(tmp,0,255,cv2.THRESH_BINARY)\n",
    "    b, g, r = cv2.split(src)\n",
    "    rgba = [b,g,r, alpha]\n",
    "    dst = cv2.merge(rgba,4)\n",
    "    return dst\n",
    "    \n",
    "def get_model_instance_segmentation(num_classes, pre_trained: bool):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    if pre_trained:\n",
    "        model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights='COCO_V1')\n",
    "    else:\n",
    "        model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2()\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
    "        in_features_mask,\n",
    "        hidden_layer,\n",
    "        num_classes\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    if train:\n",
    "        transforms.append(T.RandomPhotometricDistort(p=0.5))\n",
    "        transforms.append(T.RandomAutocontrast(p=0.5))\n",
    "        transforms.append(T.RandomAdjustSharpness(p=0.5, sharpness_factor=2))\n",
    "        transforms.append(T.RandomHorizontalFlip(p=0.5))\n",
    "        transforms.append(T.RandomVerticalFlip(p=0.5))\n",
    "    transforms.append(T.ToDtype(torch.float32, scale=True))\n",
    "    transforms.append(T.ToPureTensor())\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "\n",
    "def get_transform_object():\n",
    "    transforms = []\n",
    "    transforms.append(T.RandomRotation(degrees=180))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de710100-9911-4d8c-b20b-0b48d95946bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mask_images(path_to_model: str, output_dir: str, image_path: str, score_threshold: float, mask_threshold: float,  name: str):\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    model = get_model_instance_segmentation(2, False)\n",
    "    model.load_state_dict(torch.load(path_to_model, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = tv_tensors.Image(image)\n",
    "\n",
    "    eval_transform = get_transform(train=False)\n",
    "    with torch.no_grad():\n",
    "        x = eval_transform(image)\n",
    "        # convert RGBA -> RGB and move to device\n",
    "        x = x[:3, ...].to(device)\n",
    "        predictions = model([x, ])\n",
    "        pred = predictions[0]\n",
    "\n",
    "\n",
    "    masks = pred['masks'].cpu().numpy()\n",
    "    boxes = pred['boxes'].cpu().numpy()\n",
    "    scores = pred['scores'].cpu().numpy()\n",
    "\n",
    "\n",
    "    image = image.cpu().numpy()\n",
    "    image = np.transpose(image, (1, 2, 0))\n",
    "\n",
    "    for i in range(len(masks)):\n",
    "        if scores[i] >= score_threshold:\n",
    "            mask = masks[i, 0]\n",
    "            box = boxes[i]\n",
    "            \n",
    "            extracted_obj = cv2.bitwise_and(image, image, mask=(mask > mask_threshold).astype(np.uint8))\n",
    "            extracted_obj = extracted_obj[int(box[1]):int(box[3]), int(box[0]):int(box[2])]\n",
    "            extracted_obj = clean_background(extracted_obj)\n",
    "\n",
    "            output_path = os.path.join(output_dir, f'{name.split(\".\")[0]}_{i}.png')\n",
    "            cv2.imwrite(output_path, extracted_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "787d36a7-642d-4777-ba3f-8f299d9c8000",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dir = ''\n",
    "path_to_model = 'models/final_model_v5'\n",
    "input_dir = 'data_examples'\n",
    "output_dir = 'segmentation_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "18b3b360-203a-4d8b-85d2-c32e8d52b9b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00333af4c2fe41fdacf3c0c54f4d497a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for name in tqdm(os.listdir(input_dir)):\n",
    "    image_path = os.path.join(input_dir, name)\n",
    "\n",
    "    score_threshold = 0.9\n",
    "    mask_threshold = 0.9\n",
    "    extract_mask_images(path_to_model, output_dir, image_path, score_threshold, mask_threshold, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b06c08-5518-4a3c-9374-aa1e83aa605b",
   "metadata": {},
   "source": [
    "## Feature-extracting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3ec07a30-bef3-4363-8f8e-2b739a7713c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label_to_similarity(normed_feature: Tensor, label: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "    similarity_matrix = normed_feature @ normed_feature.transpose(1, 0)\n",
    "    label_matrix = label.unsqueeze(1) == label.unsqueeze(0)\n",
    "\n",
    "    positive_matrix = label_matrix.triu(diagonal=1)\n",
    "    negative_matrix = label_matrix.logical_not().triu(diagonal=1)\n",
    "\n",
    "    similarity_matrix = similarity_matrix.view(-1)\n",
    "    positive_matrix = positive_matrix.view(-1)\n",
    "    negative_matrix = negative_matrix.view(-1)\n",
    "    return similarity_matrix[positive_matrix], similarity_matrix[negative_matrix]\n",
    "\n",
    "\n",
    "class CircleLoss(nn.Module):\n",
    "    def __init__(self, m: float, gamma: float) -> None:\n",
    "        super(CircleLoss, self).__init__()\n",
    "        self.m = m\n",
    "        self.gamma = gamma\n",
    "        self.soft_plus = nn.Softplus()\n",
    "\n",
    "    def forward(self, sp: Tensor, sn: Tensor) -> Tensor:\n",
    "        ap = torch.clamp_min(- sp.detach() + 1 + self.m, min=0.)\n",
    "        an = torch.clamp_min(sn.detach() + self.m, min=0.)\n",
    "\n",
    "        delta_p = 1 - self.m\n",
    "        delta_n = self.m\n",
    "\n",
    "        logit_p = - ap * (sp - delta_p) * self.gamma\n",
    "        logit_n = an * (sn - delta_n) * self.gamma\n",
    "\n",
    "        loss = self.soft_plus(torch.logsumexp(logit_n, dim=0) + torch.logsumexp(logit_p, dim=0))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3f890e32-31d2-414b-8c2f-b70a1212a5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, embeddings=64):\n",
    "        super().__init__()\n",
    "        self.model = models.resnet18(weights='DEFAULT')\n",
    "        num_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Identity()\n",
    "        self.embeddings = nn.Linear(num_features, embeddings)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.embeddings(x)\n",
    "        x = nn.functional.normalize(x) # L2 normalization to put all values on a sphere\n",
    "        return x\n",
    "\n",
    "class PollenEmbeddingsModule(L.LightningModule):\n",
    "    def __init__(self, check_val=5, config={}):\n",
    "        super().__init__()\n",
    "        embeddings_size = 256\n",
    "        self.model = EmbeddingModel(embeddings_size)\n",
    "        config[\"backbone\"]=\"resnet50\"\n",
    "        config[\"embeddings_size\"]=embeddings_size\n",
    "        config[\"check_val\"]=check_val\n",
    "        config[\"optim_lr\"]=0.0001\n",
    "        config[\"optim_betas\"]=(0.9, 0.999)\n",
    "        config[\"optim_eps\"]=1e-08\n",
    "        config[\"optim_weight_decay\"]=0\n",
    "        # call this to save (arguments) to the checkpoint\n",
    "        self.save_hyperparameters(config)\n",
    "\n",
    "        self.loss_function = CircleLoss(m=0.6, gamma=80)\n",
    "        self.best_score = 1\n",
    "        self.best_val_epoch = -1\n",
    "\n",
    "        self.train_step_outputs = []\n",
    "        self.val_step_outputs = []\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.model(inputs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        img, label, _ = batch\n",
    "        pred = self.model(img)\n",
    "        loss = self.loss_function(*convert_label_to_similarity(pred, label))\n",
    "        pred = {\"train_loss\": loss, \"train_number\": len(pred)}\n",
    "        self.train_step_outputs.append(pred)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        outputs = self.train_step_outputs\n",
    "        train_loss, num_items = 0, 0\n",
    "        for output in outputs:\n",
    "            train_loss += output[\"train_loss\"].sum().item()\n",
    "            num_items += output[\"train_number\"]\n",
    "\n",
    "        mean_train_loss = torch.tensor(train_loss / num_items)\n",
    "        self.log(\"train_loss\", mean_train_loss)\n",
    "        self.train_losses.append(mean_train_loss.item())\n",
    "        self.train_step_outputs.clear()  # free memory\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        img, label, _ = batch\n",
    "        pred = self.model(img)\n",
    "        loss = self.loss_function(*convert_label_to_similarity(pred, label))\n",
    "        pred = {\"val_loss\": loss, \"val_number\": len(pred)}\n",
    "        self.val_step_outputs.append(pred)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        outputs = self.val_step_outputs\n",
    "        val_loss, num_items = 0, 0\n",
    "        for output in outputs:\n",
    "            val_loss += output[\"val_loss\"].sum().item()\n",
    "            num_items += output[\"val_number\"]\n",
    "\n",
    "        mean_val_loss = torch.tensor(val_loss / num_items)\n",
    "        self.log(\"val_loss\", mean_val_loss)\n",
    "        self.val_losses.append(mean_val_loss.item())\n",
    "        self.val_step_outputs.clear()  # free memory\n",
    "\n",
    "        if mean_val_loss < self.best_score:\n",
    "            self.best_score = mean_val_loss\n",
    "            self.best_val_epoch = self.current_epoch\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(),\n",
    "                                          lr=self.hparams.optim_lr,\n",
    "                                          betas=self.hparams.optim_betas,\n",
    "                                          eps=self.hparams.optim_eps,\n",
    "                                          weight_decay=self.hparams.optim_weight_decay)\n",
    "        return {\n",
    "            \"optimizer\": self.optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": ReduceLROnPlateau(self.optimizer, factor=0.1, patience=10),\n",
    "                \"frequency\": self.hparams.check_val,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"interval\": \"epoch\",\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def plot_losses(self):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot([i for i in range(len(self.train_losses))], self.train_losses, label='Train Loss')\n",
    "        plt.plot([i * self.hparams.check_val for i in range(len(self.val_losses)-1)], self.val_losses[1:], label='Validation Loss')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Losses')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "6ff62b6f-8c4e-49bd-b080-1cdae584a88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'segmentation_output'\n",
    "checkpoint_path = \"/home/jovyan/git/Pollen-Analyzer/Classification/history/checkpoint-epoch=11-val_loss=0.5668.ckpt\"\n",
    "loaded_model = PollenEmbeddingsModule.load_from_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "7d0585d4-63e3-4fe0-bd59-af7f7dde287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.eval()\n",
    "loaded_model = loaded_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "2bc06ae0-8b94-4ce8-a406-41c29fe33f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad2size(image, size=(768, 768)):\n",
    "    return ImageOps.fit(image, size)\n",
    "\n",
    "static_transforms = v2.Compose([\n",
    "    # v2.Lambda(pad2size),\n",
    "    v2.PILToTensor(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def extract_embiddings(model, image_path):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img = static_transforms(img)\n",
    "    img = img[None, :, :, :]\n",
    "    img = img.cuda()\n",
    "    embeddings = model(img)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "c4325199-80ef-42cd-96c7-85f728b3de86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c0cdea14e14ac3ad8cc0065f76576d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings_data = []\n",
    "labels_data_str = []\n",
    "images = glob(os.path.join(input_dir, \"*.png\"))\n",
    "\n",
    "for image_path in tqdm(images):\n",
    "    embeddings_data.append(extract_embiddings(loaded_model, image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "886a23cd-f2df-467a-b721-013b667205e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_class_markings(ax, class_boundaries, labels):\n",
    "    tick_locs = [(class_boundaries[i] + class_boundaries[i+1] - 1) / 2 for i in range(len(class_boundaries)-1)]\n",
    "    ax.set_xticks(tick_locs)\n",
    "    ax.set_yticks(tick_locs)\n",
    "    ax.set_xticklabels([labels[int(loc)] for loc in tick_locs])\n",
    "    ax.set_yticklabels([labels[int(loc)] for loc in tick_locs])\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    for boundary in class_boundaries[1:-1]:\n",
    "        ax.axhline(y=boundary-1, color='k', linestyle='-', linewidth=1)\n",
    "        ax.axvline(x=boundary-1, color='k', linestyle='-', linewidth=1)\n",
    "\n",
    "def prepare_data(embeddings_lists: list, labels_lists: list):\n",
    "    all_labels_sorted = []\n",
    "    all_embeddings_sorted = []\n",
    "    class_boundaries = [0]\n",
    "    for i, embeddings_list in enumerate(embeddings_lists):\n",
    "        labels_list = labels_lists[i]\n",
    "        class_to_idx = {}\n",
    "        for i, class_name in enumerate(np.unique(labels_list)):\n",
    "            class_to_idx[class_name] = i\n",
    "        all_labels_idx = [class_to_idx[label] for label in labels_list]\n",
    "        sorted_indices = sorted(range(len(embeddings_list)), key=lambda i: all_labels_idx[i])\n",
    "        start = len(all_labels_sorted) + 1\n",
    "        all_embeddings_sorted.extend([embeddings_list[i] for i in sorted_indices])\n",
    "        all_labels_sorted.extend([labels_list[i] for i in sorted_indices])\n",
    "    \n",
    "        for i in range(start, len(all_labels_sorted)):\n",
    "            if all_labels_sorted[i] != all_labels_sorted[i-1]:\n",
    "                class_boundaries.append(i)\n",
    "        class_boundaries.append(len(all_labels_sorted))\n",
    "\n",
    "    return all_labels_sorted, all_embeddings_sorted, class_boundaries\n",
    "    \n",
    "def plot_matrix(embeddings_lists: list, labels_lists: list, subtitle: str=None, axes_class_markings: bool=True):\n",
    "    if len(labels_lists) != 0:\n",
    "        all_labels_sorted, all_embeddings_sorted, class_boundaries = prepare_data(embeddings_lists, labels_lists)\n",
    "        pairwise_dist = pairwise_distances(all_embeddings_sorted)\n",
    "    else:\n",
    "        axes_class_markings = False\n",
    "        embeddings_lists = [i.detach().cpu().numpy()[0] for i in embeddings_lists]\n",
    "        pairwise_dist = pairwise_distances(embeddings_lists)\n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "    im = ax.imshow(pairwise_dist, cmap='Blues_r')\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "\n",
    "    if axes_class_markings:\n",
    "        add_class_markings(ax, class_boundaries, all_labels_sorted)\n",
    "\n",
    "    if subtitle != None:\n",
    "        plt.title('Pairwise Distance Matrix\\n' + subtitle)\n",
    "    else:\n",
    "        plt.title('Pairwise Distance Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    if len(labels_lists) != 0:\n",
    "        del all_labels_sorted\n",
    "        del all_embeddings_sorted\n",
    "        del class_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "3c182287-6ec5-4c11-8543-688140987e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_matrix(embeddings_data, labels_data_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66085ea1-b882-4928-b1db-49471ce323e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
